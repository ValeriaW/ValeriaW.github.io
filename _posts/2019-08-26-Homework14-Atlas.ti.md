---
layout: post
title: Qualitative Data Analysis with Atlas.ti
---


My fourteenth homework post dealing with the qualitative data analysis programme Atlas.ti!
<!-- more -->

# Homework 14

***

My personal background is actually not history, but it is global studies and more specifically political psychology. Naturally, qualitative data analysis and so-called 'thick data' are a huge wealth of information that I want to compliment with big data. Why do I think the two go hand in hand?

Atlas.ti is a qualitative data analysis programme that allows for the coding of documents, interviews, audio files, videos, and images. It further allows you to code twitter data and the georeferencing of data as well. The coded data can then be used to create networks, show the relationship between the codes and combine all kinds of interconnected insights into one report. 

![exampleimages](/img/Imagetable.png)

Above, an example of coded images exported as a table. 

For the humanities in general, qualitative data analysis is a staple and the complementary coding, and synthesising of conclusions is invaluable. The atlas.ti programme is also simply a great tool to create literature reviews and overviews of many documents on one specific topic, as is required for a PhD or a journal article.

It facilitates the discovery of patterns and makes analysis across different formats (pdf, video, images, twitter, geodata, networks, audio) easy in one place. Finally, it helps to generate useful data visualizations for a report, thesis or journal article.

You can collaborate with other people and work with large amounts of data. However, it has some limitations when it comes to freely adjusting data you would like to pull from twitter hashtags etc. The ease with which tables are generated and exported make it a seemless bridge to further analysis in other programmes.

Even though it was created with regard to qualitative data analysis, the tool has evolved and you can combine qual and quant (for example survey evaluation), with the main focus on assisting in the creation of a report and the export of data tables to be used in other programmes such as SPSS.

You can further import complete codebooks from previous research, to build upon deductive insights but also work from the 'bottom-up' and create codes from research in an inductive approach.
 
# Integration of Thick Data and Big Data

In the social sciences, a historical over-dependence on quantitative data, as the only empirical source of data, has stifled the analysis of dynamic, non-linear systems phenomena. The recognition of the complex dynamic systems paradigm, wherin open systems require holistic analysis, gave way to new methodologies and in a way re-legitimized qualitative data analysis as a valuable source of data and context for systems modeling in combination with quantitative insights (Flaherty, 2019).

Thick data is data from humans that cannot be quantified, has a small sample size, but delivers depth of meaning and context to quantified static data analysis when it is applied to dynamic systems.

Big data is good at making predictions for contained (almost static) systems, however complex *dynamic* systems, especially systems involving human beings that are always non-linear need to be complemented by thick data. On top of that, complex systems analysis that can be achieved by programmes such as the agent-based modeling software *Netlogo*. 

The dataset we worked with during the course was very big and the idea of the tools we were introduced to was to use superior scope of automated working processes to analyse data that go beyond what an individual researcher could achieve

I think it’s important to distinguish between ‘big data’ and data science more broadly. The latter refers to computationally powered quantitative analysis and modelling techniques, even when applied to ‘smaller’ data sets, while the former is over- and misused to describe exactly that. 

# Atlas.ti 

In that vein, analysing twitter data has become an interesting field of research in sociology, psychology and political science among other disciplines. In the qualitative data analysis tool atlas.ti, one can simply import twitter data, under the folder 'documents' and then import the set number of tweets. There is a huge drawback however, and that is the limitation that yuo can only import tweets from the past 7 days. I think this is a major flaw that severely incapacitates the utility of this tool.

![twitterimport](/img/Tweetimport.png)

The import will produce a single document with all tweets and a set of codes for the hashtags, the author, the language of the tweet and the location from where the tweet was made. The tweets themselves become quotations. So, for example, all of the tweets under the hashtag ex: ‘#DigitalHumanities’ will be quotations linked to that code. One quotation will have several codes, and each retweet is a connection. All data that is contained in one quotation can be visualized using a network of nodes, that represent different pieces of information. The relationship between each node can be set in the link manager, e.g. explains, reacts to, succeeds etc.

With the example of #ethnography, which yielded more results for the last 7 days than #digital humanities, atlas.ti created this table. All the data can be easily exported in csv format or other, to be used in gephi or further structured using python, e.g. adding coordinates to the locations of the tweets.

# Step by Step guide for coding a literature review


![Table](/img/Table_Atlas.ti.png)

This is an example from a literature review I had to write for a philosophy class about migration as a global public good "Migration and Distritubive Justice". I coded eight documents in Atlas.ti, figuring out along the way what the important codes were, that I could then later use as subheadings for the literature review. The content codes thus provide a logical structure centered around topics instead of discussing each paper in sequence. For more abstract texts, coding by hand can be the only way to go about it as topic modeling would only yield a limited accuracy. Yet it could provide an initial stepping stone, when the number of documents is very large. 

To write a literature review can be seen as a data collection tool. It can be standalone research, or used to inform primary research. In the case of the example lit review (LR), we were supposed to review no more than 10 documents on a topic of distributive justice.

(1) Select and import documents

Once the documents for the LR are selected, you can open a new work space in Atlas.ti and import the documents by clicking import documents in the top left corner.

![Step1](/img/Step1.png)

(2) Reading the text: Coding, Quotations, Comments

You can read the documents in atlas and code them as you go. To code, select the sentence, paragraph or word that you would like to code, in the case of the example, I coded MDF which stands for Migration Distributive Framework because the snippet I coded is relevant for that cluster to be debated in the literature review. At first, it is challenging to not be too specific while coding but also not to general. It depends on the preference of the researcher, some people like to "chunk" some people like to "splice" and provide a code for very specific pieces of data. Of course, when the codes exceed 100, it is time to see if you can merge codes, or why such an excessibe number of codes is emerging.I prefer to stay as close to the expected subheadings of a LR as possible. Personally, I like to brainstorm before I have read the texts what codes I am expecting and later on I can always amend them as I go. 

The first document is always the hardest, because you need to decide what codes will form the basis for the coding of all other texts.

![Step2](/img/Step2Coding.png)

![Step22](/img/Step2-extended.png)
Quotations and Codes

Unfortunately, my programme is in German, but when you right-click, the top option says "create quotation", short cut cmd+ H and then second option from the top is cmd + J and is "create code". To create a quotation is useful, if something is important but you do not yet have an idea of how to code it or it is an important piece of information about the text in general, simply mark it as a quote.

Several codes are always useful when creating an LR:
* Thesis Statement
* Aim of the text
* Hypothesis (if applicable)

Further, to the right of the text, you can create a comment about the whole document and summarize the relevance the text bears to your research, or a quick summary of the text and how it links to the other documents. 

![Comment](/img/Commentsection.png)


(3) After coding: analysis tools

Entity managers: After coding, you can go to the code manager (see image below), where all the coded comments are listed by code and you can create a table of all the codes or select the onces you want to compare in Excel or a report in word (see second picture)

![Codemanager](/img/Codemanager.png)
Codemanager

![Createreporttable](/img/Report:Table.png)
Creating a report or a table.

Code table:
![table](/img/Tableee.png)

The table can be used to create a nodes and edges table for Gephi for example.

A code document table serves the purpose of visualising the occurence codes in specific documents. As can be seen in the one I created for the example LR, I have coded the first document a lot but the codes do not appear as often in the subsequent documents. That is because many times, codes become obsolete (they should have general clustering power) when they simply do not apply in the other documents, as arguments on the same topic can be framed very differently.

![Codedocument](/img/Code-document-table.png)
Code-document table

To create a network, simply click Networks > New Network and drag the desired entities (documents, codes or single quotations into the white space (see image below).

![network](/img/Network.png)
Network

I find it very useful for myself, simply to visualize logical connections.
Networks become more sophisticated, the more information you add, such as for example grouping codes. 

I have grouped the codes into three clusters in this case:

1. Global Justice
2. Migration policy
3. Migration Theory

It can be useful when creating a network, 

![Alles](/img/Alles.png)

In the above screenshot you can see the network I created on the basis of the 19 codes that I put into 3 code groups, the 8 documents that I coded. I focused solely on the codes for the network, as it then formed the basis for my LR. 

# Alternatives to Atlas.ti

The main rival of Atlas.ti is MaxQDA. Both try to establish themselves as the number 1 qualitative data analysis tool. There is also Nvivo. The reason why I use atlas.ti is because it was highly recommended to me by a professor at Stellenbosch University and it requires very little time to learn, as it is very intuitive. 

As I have not used the other QDA software packages,  I can only speak to the pro's and con's of Atlas.ti.

After using it for several different purposes (Interview analysis, Programme Evaluation, Literature Reviews), the main advantage was the versatility and simple logic of content coding. Originally, I had been looking for a QDA software to analyse interviews. One of the most annoying tasks is the transcription of interviews that can take a lot of time. With atlas.ti it is possible to simply code the audio file directly and only transcribe already coded parts of the interviews. That alone was a game-changer for my bachelor thesis. Another pro is the content focused way to read documents in general. I use it for all kinds of assigned reading because later on, I can just pull a report based on the codes I made along the way, much like I would have done previously by re-visiting highlighted sections of a text. 
Further, the software programme has come a long way and the developers are constantly improving and amending the abilities of the programme.

Disadvantages of atlas.ti are the limitations that features have. The twitter import is an amazing tool, yet, it is limited to tweets made in the past 7 days. Overall, for each feature there exists a programme that can perform the tasks more sophisticatedly (ex:Creation of networks/data visualizations in Gephi). Another disadvantage is the price of the software.

# Limitations vs. Freedom 

Once a researcher becomes skilled at programming, limitations of software programmes, that made analysis tools for non-IT-versed researchers more accessible, can indeed restrict the possibilities programming has to offer for the humanities. Then open source software and using  R and Python among other things, can open up whole new avenues to analysing data.

However, for many researchers, the digital humanities are still just out of reach and it is easier to strucutre data in more accessible formats like this (similar to Gephi in a way). That is why I wanted to share and add Atlas.ti to (my personal) course curriculum.


# References

Flaherty, E. (2019). Complexity and Resilience in the Social and Ecological Sciences. Palgrave Macmillan UK.





***
